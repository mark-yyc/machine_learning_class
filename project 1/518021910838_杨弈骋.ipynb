{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# machine learning-project1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data and split  \n",
    "use train_test_split in sklearn to split data randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def loadDataSet():\n",
    "    data = pd.read_csv(\"./train.csv\")\n",
    "    convertLabel(data)\n",
    "    y = data['price_range']\n",
    "    x = data.drop('price_range', axis = 1)\n",
    "    x_train, x_tmp, y_train, y_tmp = train_test_split(x, y, test_size = 0.2, random_state = 60, stratify = y)\n",
    "    x_test, x_valid, y_test, y_valid = train_test_split(x_tmp, y_tmp, test_size = 0.5, random_state = 100, stratify = y_tmp)\n",
    "    return x_train,y_train,x_test,y_test,x_valid,y_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess data  \n",
    "convertLabel: Convert labels into to two classes: low (0, 1) and high (2, 3)  \n",
    "DataLabelSplit(): split the data into two part, one labeled high price_range, the other labeled low price_range  \n",
    "normalize(): used for Logistic Regression, scaling values of attributes to the same level [0,1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertLabel(data):\n",
    "    data.loc[data.price_range<=1,'price_range']=0\n",
    "    data.loc[data.price_range>1,'price_range']=1\n",
    "\n",
    "\n",
    "def DataLabelSplit(x_data,y_data):\n",
    "    x=x_data.values\n",
    "    y=y_data.values\n",
    "    data=np.column_stack((x,y))\n",
    "    true_data=data[data[:,20]==1]\n",
    "    false_data=data[data[:,20]==0]\n",
    "    return true_data,false_data\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    column_num=x.shape[1]\n",
    "    row_num=x.shape[0]\n",
    "    array=x.values\n",
    "    for i in range(column_num):\n",
    "        column_max=max(array[:,i])\n",
    "        array[:,i]=array[:,i]/column_max\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model and algorithm  \n",
    "note: function train_and_test below produce the entire procedure of training and testing  \n",
    "1. Naive Bayes  \n",
    "instead of discretizing continuous attributes into intervals, I use Gauss probability density to measure the probability for continuous attributes\n",
    "getMeanStdLabel(),calcuGaussProb(): used for calculate Gauss probability density function\n",
    "calPossibilityForDiscrete(): calculate the probability for desicrete attributes\n",
    "calPossibility(): calculate the entire probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMeanStdLabel(true_data,false_data):\n",
    "    return np.mean(true_data,0), np.std(true_data,  0 ),np.mean(false_data,0), np.std(false_data,  0 )\n",
    "\n",
    "\n",
    "def calPossibilityForDiscrete(true_data,false_data):\n",
    "    column_number=true_data.shape[1]\n",
    "    true_data_possibility=[]\n",
    "    false_data_possibility=[]\n",
    "    for i in range(column_number):\n",
    "        true_data_factor_num=true_data[true_data[:,i]>0].shape[0]\n",
    "        true_data_possibility.append(true_data_factor_num/true_data.shape[0])\n",
    "        false_data_factor_num=false_data[false_data[:,i]>0].shape[0]\n",
    "        false_data_possibility.append(false_data_factor_num/false_data.shape[0])\n",
    "    return true_data_possibility,false_data_possibility\n",
    "\n",
    "\n",
    "def calcuGaussProb(x,mean,stdev):\n",
    "    exponent = np.exp(-(np.power(x-mean,2))/(2*np.power(stdev,2)))\n",
    "    GaussProb = (1/(np.sqrt(2*np.pi)*stdev))*exponent\n",
    "    return GaussProb\n",
    "\n",
    "\n",
    "def calPossibility(arr,mean_arr,std_arr,pro_arr):\n",
    "    possibility=1\n",
    "    for i in range(len(arr)):\n",
    "        if i==1 or i==3 or i==5 or i==17 or i==18 or i==19:\n",
    "            if arr[i]==1:\n",
    "                possibility*=pro_arr[i]\n",
    "            else:\n",
    "                possibility*=1-pro_arr[i]\n",
    "        else:\n",
    "            possibility*=calcuGaussProb(arr[i],mean_arr[i],std_arr[i])\n",
    "    return possibility\n",
    "\n",
    "\n",
    "def naive_bayes_predict(x_train,y_train):\n",
    "#     x_train,y_train,x_test,y_test,x_valid,y_valid=loadDataSet()\n",
    "    true_data,false_data=DataLabelSplit(x_train,y_train)\n",
    "    true_mean,true_std,false_mean,false_std=getMeanStdLabel(true_data,false_data)\n",
    "    true_data_possibility,false_data_possibility=calPossibilityForDiscrete(true_data,false_data)\n",
    "    return true_mean,true_std,false_mean,false_std,true_data_possibility,false_data_possibility\n",
    "\n",
    "\n",
    "def test_bayes(true_mean,true_std,false_mean,false_std,true_data_possibility,false_data_possibility,x_test,y_test):\n",
    "    prediction=[]\n",
    "    for i in range(x_test.shape[0]):\n",
    "        arr=x_test.values[i]\n",
    "        true_possibility=calPossibility(arr,true_mean,true_std,true_data_possibility)\n",
    "        false_possibility=calPossibility(arr,false_mean,false_std,false_data_possibility)\n",
    "        if true_possibility>false_possibility:\n",
    "            prediction.append(1)\n",
    "        else:\n",
    "            prediction.append(0)\n",
    "#     print(prediction)\n",
    "    result=y_test.values\n",
    "    count=0\n",
    "    for i in range(len(result)):\n",
    "        if prediction[i]==result[i]:\n",
    "            count+=1\n",
    "    return count/len(result)\n",
    "\n",
    "def bayes_train_and_test(x_train,y_train,x_test,y_test,x_valid,y_valid):\n",
    "    true_mean,true_std,false_mean,false_std,true_data_possibility,false_data_possibility=naive_bayes_predict(x_train,y_train)\n",
    "    return test_bayes(true_mean,true_std,false_mean,false_std,true_data_possibility,false_data_possibility,x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Logistic Regression  \n",
    "logisticRegression(): use gradient descend to train w  \n",
    "test(): test valid data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(gamma):\n",
    "#     print(\"sigmoid(%d)\" % gamma)\n",
    "    if gamma < 0:\n",
    "        return 1 - 1/(1 + math.exp(gamma))\n",
    "    else:\n",
    "        return 1/(1 + math.exp(-gamma))\n",
    "\n",
    "    \n",
    "def initW(data_num):\n",
    "    w=[]\n",
    "    for i in range(data_num):\n",
    "        w.append(random.uniform(-0.01,0.01))\n",
    "    return np.array(w)\n",
    "\n",
    "def test_logisticRegression(w,x,y):\n",
    "    row_num=x.shape[0]\n",
    "    column_num=x.shape[1]\n",
    "    data=normalize(x)\n",
    "    prediction=[]\n",
    "    for i in range(row_num):\n",
    "        row=np.append(1,data[i])\n",
    "        if w.dot(row)>0:\n",
    "            prediction.append(1)\n",
    "        else:\n",
    "            prediction.append(0)\n",
    "    \n",
    "    result=y.values\n",
    "    count=0\n",
    "    for i in range(len(result)):\n",
    "        if prediction[i]==result[i]:\n",
    "            count+=1\n",
    "#     print(count/len(result))\n",
    "    return count/len(result)\n",
    "\n",
    "\n",
    "def logisticRegression(x_train,y_train,x_valid,y_valid):\n",
    "    ita=0.002\n",
    "#     x_train,y_train,x_test,y_test,x_valid,y_valid=loadDataSet()\n",
    "    column_num=x_train.shape[1]\n",
    "    train_num=x_train.shape[0]\n",
    "    w=initW(column_num+1)\n",
    "    data_train=normalize(x_train)\n",
    "    for i in range(500):\n",
    "        deltaW=np.zeros(column_num+1)\n",
    "        for t in range(train_num):\n",
    "            row=np.append(1,data_train[t])\n",
    "            y=sigmoid(w.dot(row))\n",
    "            error=y_train.values[t]-y\n",
    "            deltaW+=error*row\n",
    "        w+=ita*deltaW\n",
    "        if test_logisticRegression(w,x_valid,y_valid)>=0.95:\n",
    "            break\n",
    "    return w\n",
    "\n",
    "\n",
    "def logisticRegression_train_and_test(x_train,y_train,x_test,y_test,x_valid,y_valid):\n",
    "    w=logisticRegression(x_train,y_train,x_valid,y_valid)\n",
    "    return test_logisticRegression(w,x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. svm  \n",
    "use module svm in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "\n",
    "def svmPredict(x_train,y_train):\n",
    "#     x_train,y_train,x_test,y_test,x_valid,y_valid=loadDataSet()\n",
    "    clf = svm.SVC(C=1,decision_function_shape='ovo')\n",
    "    clf.fit(x_train,y_train)\n",
    "    return clf\n",
    "\n",
    "\n",
    "def svm_test(clf,x_test,y_test):\n",
    "    return clf.score(x_test,y_test)\n",
    "\n",
    "\n",
    "def svm_train_and_test(x_train,y_train,x_test,y_test,x_valid,y_valid):\n",
    "    clf=svmPredict(x_train,y_train)\n",
    "    return svm_test(clf,x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical Study\n",
    "1. comparison of accuracy  \n",
    "discription:  \n",
    "+ figureTableAccuracy below display the accuracy on the training set and the test set separately,which will print out a bar chart and a table\n",
    "+ from the chart, we can clearly figure out although svm model is the most accurate, results for three models are closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>training set</th>\n",
       "      <th>test set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>naive-bayes</th>\n",
       "      <td>0.93875</td>\n",
       "      <td>0.930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic regression</th>\n",
       "      <td>0.96250</td>\n",
       "      <td>0.975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>0.98125</td>\n",
       "      <td>0.985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     training set  test set\n",
       "naive-bayes               0.93875     0.930\n",
       "logistic regression       0.96250     0.975\n",
       "svm                       0.98125     0.985"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def figureTableForAccuracy():\n",
    "    name_list= ['training set','test set']\n",
    "    x_train,y_train,x_test,y_test,x_valid,y_valid=loadDataSet()\n",
    "    bayes=[]\n",
    "    rl=[]\n",
    "    svm=[]\n",
    "\n",
    "    true_mean,true_std,false_mean,false_std,true_data_possibility,false_data_possibility=naive_bayes_predict(x_train,y_train)\n",
    "    bayes.append(test_bayes(true_mean,true_std,false_mean,false_std,true_data_possibility,false_data_possibility,x_train,y_train))\n",
    "    bayes.append(test_bayes(true_mean,true_std,false_mean,false_std,true_data_possibility,false_data_possibility,x_test,y_test))\n",
    "\n",
    "    w=logisticRegression(x_train,y_train,x_valid,y_valid)\n",
    "    rl.append(test_logisticRegression(w,x_train,y_train))\n",
    "    rl.append(test_logisticRegression(w,x_test,y_test))\n",
    "\n",
    "    clf=svmPredict(x_train,y_train)\n",
    "    svm.append(svm_test(clf,x_train,y_train))\n",
    "    svm.append(svm_test(clf,x_test,y_test))\n",
    "\n",
    "    x =list(range(len(bayes))) \n",
    "    total_width, n = 0.6, 3\n",
    "    width = total_width / n\n",
    "\n",
    "    plt.bar(x, bayes, width=width, label='naive-bayes',fc = 'y')  \n",
    "    for i in range(len(x)):  \n",
    "        x[i] = x[i] + width\n",
    "    plt.bar(x, rl, width=width, label='logistic regressio',tick_label = name_list,fc = 'r')\n",
    "    for i in range(len(x)):  \n",
    "        x[i] = x[i] + width\n",
    "    plt.bar(x, svm, width=width, label='svm',fc = 'b')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    table=np.reshape(bayes+rl+svm,(3,2))\n",
    "    return pd.DataFrame(table,index=['naive-bayes','logistic regression','svm'],columns=['training set','test set'])\n",
    "\n",
    "figureTableForAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. comparison of runtime  \n",
    "discription:  \n",
    "+ function displayTime() can print a table, which record runtime for three models to train the data and test accuracy\n",
    "+ from the table, we can figure out that cost for naive-bayes and svm is low, and the time for logistic regression is much longer. The reason is that I use BGD instead of SGD to train w, so there are 1600 rows of data to use every cycle.\n",
    "+ the reason for BGD instead of SGD is to make variable stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>naive-bayes</th>\n",
       "      <td>0.065351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic regression</th>\n",
       "      <td>8.266150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>0.016307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         time\n",
       "naive-bayes          0.065351\n",
       "logistic regression  8.266150\n",
       "svm                  0.016307"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def displayTime():\n",
    "    x_train,y_train,x_test,y_test,x_valid,y_valid=loadDataSet()\n",
    "    runtime=[]\n",
    "    start =time.time()\n",
    "    bayes_train_and_test(x_train,y_train,x_test,y_test,x_valid,y_valid)\n",
    "    end = time.time()\n",
    "    runtime.append(end-start)\n",
    "\n",
    "    start =time.time()\n",
    "    logisticRegression_train_and_test(x_train,y_train,x_test,y_test,x_valid,y_valid)\n",
    "    end = time.time()\n",
    "    runtime.append(end-start)\n",
    "\n",
    "    start =time.time()\n",
    "    svm_train_and_test(x_train,y_train,x_test,y_test,x_valid,y_valid)\n",
    "    end = time.time()\n",
    "    runtime.append(end-start)\n",
    "\n",
    "    return pd.DataFrame(runtime,index=['naive-bayes','logistic regression','svm'],columns=['time'])\n",
    "\n",
    "displayTime()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Analysis\n",
    "+ naive-bayes: use bayes function to measure the probability \n",
    "pros: the algorithm is simple, it just use statistic and calculation, instead of training variable with while loop\n",
    "cons: because the model ignored the dependency in every two factors, so the accuracy is not very high. When some factors have dependency to other factor, the model will fail to classify the data\n",
    "\n",
    "+ logistic regression: use sigmoid and GD to train f(x)=WX+b\n",
    "pros: the usage for this model does not have many limit like naive-bayes, and the accuracy tend to be pleasing\n",
    "cons: cost is high, GD is a costly process\n",
    "\n",
    "+ svm\n",
    "pros: the usage is very universal and the cost is not so high\n",
    "this time, I use sklearn to use svm model, so I cannot figure out the cons"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
