{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# machine learning-project1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data and split  \n",
    "use train_test_split in sklearn to split data randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def loadDataSet():\n",
    "    data = pd.read_csv(\"./train.csv\")\n",
    "    convertLabel(data)\n",
    "    y = data['price_range']\n",
    "    x = data.drop('price_range', axis = 1)\n",
    "    x_train, x_tmp, y_train, y_tmp = train_test_split(x, y, test_size = 0.2, random_state = 60, stratify = y)\n",
    "    x_test, x_valid, y_test, y_valid = train_test_split(x_tmp, y_tmp, test_size = 0.5, random_state = 100, stratify = y_tmp)\n",
    "    return x_train,y_train,x_test,y_test,x_valid,y_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess data  \n",
    "convertLabel: Convert labels into to two classes: low (0, 1) and high (2, 3)  \n",
    "DataLabelSplit(): split the data into two part, one labeled high price_range, the other labeled low price_range  \n",
    "normalize(): used for Logistic Regression, scaling values of attributes to the same level [0,1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertLabel(data):\n",
    "    data.loc[data.price_range<=1,'price_range']=0\n",
    "    data.loc[data.price_range>1,'price_range']=1\n",
    "\n",
    "\n",
    "def DataLabelSplit(x_data,y_data):\n",
    "    x=x_data.values\n",
    "    y=y_data.values\n",
    "    data=np.column_stack((x,y))\n",
    "    true_data=data[data[:,20]==1]\n",
    "    false_data=data[data[:,20]==0]\n",
    "    return true_data,false_data\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    column_num=x.shape[1]\n",
    "    row_num=x.shape[0]\n",
    "    array=x.values\n",
    "    for i in range(column_num):\n",
    "        column_max=max(array[:,i])\n",
    "        array[:,i]=array[:,i]/column_max\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model and algorithm  \n",
    "note: function train_and_test below produce the entire procedure of training and testing  \n",
    "1. Naive Bayes  \n",
    "instead of discretizing continuous attributes into intervals, I use Gauss probability density to measure the probability for continuous attributes\n",
    "getMeanStdLabel(),calcuGaussProb(): used for calculate Gauss probability density function\n",
    "calPossibilityForDiscrete(): calculate the probability for desicrete attributes\n",
    "calPossibility(): calculate the entire probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMeanStdLabel(true_data,false_data):\n",
    "    return np.mean(true_data,0), np.std(true_data,  0 ),np.mean(false_data,0), np.std(false_data,  0 )\n",
    "\n",
    "\n",
    "def calPossibilityForDiscrete(true_data,false_data):\n",
    "    column_number=true_data.shape[1]\n",
    "    true_data_possibility=[]\n",
    "    false_data_possibility=[]\n",
    "    for i in range(column_number):\n",
    "        true_data_factor_num=true_data[true_data[:,i]>0].shape[0]\n",
    "        true_data_possibility.append(true_data_factor_num/true_data.shape[0])\n",
    "        false_data_factor_num=false_data[false_data[:,i]>0].shape[0]\n",
    "        false_data_possibility.append(false_data_factor_num/false_data.shape[0])\n",
    "    return true_data_possibility,false_data_possibility\n",
    "\n",
    "\n",
    "def calcuGaussProb(x,mean,stdev):\n",
    "    exponent = np.exp(-(np.power(x-mean,2))/(2*np.power(stdev,2)))\n",
    "    GaussProb = (1/(np.sqrt(2*np.pi)*stdev))*exponent\n",
    "    return GaussProb\n",
    "\n",
    "\n",
    "def calPossibility(arr,mean_arr,std_arr,pro_arr):\n",
    "    possibility=1\n",
    "    for i in range(len(arr)):\n",
    "        if i==1 or i==3 or i==5 or i==17 or i==18 or i==19:\n",
    "            if arr[i]==1:\n",
    "                possibility*=pro_arr[i]\n",
    "            else:\n",
    "                possibility*=1-pro_arr[i]\n",
    "        else:\n",
    "            possibility*=calcuGaussProb(arr[i],mean_arr[i],std_arr[i])\n",
    "    return possibility\n",
    "\n",
    "\n",
    "def naive_bayes_predict(x_train,y_train):\n",
    "#     x_train,y_train,x_test,y_test,x_valid,y_valid=loadDataSet()\n",
    "    true_data,false_data=DataLabelSplit(x_train,y_train)\n",
    "    true_mean,true_std,false_mean,false_std=getMeanStdLabel(true_data,false_data)\n",
    "    true_data_possibility,false_data_possibility=calPossibilityForDiscrete(true_data,false_data)\n",
    "    return true_mean,true_std,false_mean,false_std,true_data_possibility,false_data_possibility\n",
    "\n",
    "\n",
    "def test_bayes(true_mean,true_std,false_mean,false_std,true_data_possibility,false_data_possibility,x_test,y_test):\n",
    "    prediction=[]\n",
    "    for i in range(x_test.shape[0]):\n",
    "        arr=x_test.values[i]\n",
    "        true_possibility=calPossibility(arr,true_mean,true_std,true_data_possibility)\n",
    "        false_possibility=calPossibility(arr,false_mean,false_std,false_data_possibility)\n",
    "        if true_possibility>false_possibility:\n",
    "            prediction.append(1)\n",
    "        else:\n",
    "            prediction.append(0)\n",
    "#     print(prediction)\n",
    "    result=y_test.values\n",
    "    count=0\n",
    "    for i in range(len(result)):\n",
    "        if prediction[i]==result[i]:\n",
    "            count+=1\n",
    "    return count/len(result)\n",
    "\n",
    "def bayes_train_and_test(x_train,y_train,x_test,y_test,x_valid,y_valid):\n",
    "    true_mean,true_std,false_mean,false_std,true_data_possibility,false_data_possibility=naive_bayes_predict(x_train,y_train)\n",
    "    return test_bayes(true_mean,true_std,false_mean,false_std,true_data_possibility,false_data_possibility,x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Logistic Regression  \n",
    "logisticRegression(): use gradient descend to train w  \n",
    "test(): test valid data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(gamma):\n",
    "#     print(\"sigmoid(%d)\" % gamma)\n",
    "    if gamma < 0:\n",
    "        return 1 - 1/(1 + math.exp(gamma))\n",
    "    else:\n",
    "        return 1/(1 + math.exp(-gamma))\n",
    "\n",
    "    \n",
    "def initW(data_num):\n",
    "    w=[]\n",
    "    for i in range(data_num):\n",
    "        w.append(random.uniform(-0.01,0.01))\n",
    "    return np.array(w)\n",
    "\n",
    "def test_logisticRegression(w,x,y):\n",
    "    row_num=x.shape[0]\n",
    "    column_num=x.shape[1]\n",
    "    data=normalize(x)\n",
    "    prediction=[]\n",
    "    for i in range(row_num):\n",
    "        row=np.append(1,data[i])\n",
    "        if w.dot(row)>0:\n",
    "            prediction.append(1)\n",
    "        else:\n",
    "            prediction.append(0)\n",
    "    \n",
    "    result=y.values\n",
    "    count=0\n",
    "    for i in range(len(result)):\n",
    "        if prediction[i]==result[i]:\n",
    "            count+=1\n",
    "#     print(count/len(result))\n",
    "    return count/len(result)\n",
    "\n",
    "\n",
    "def logisticRegression(x_train,y_train,x_valid,y_valid):\n",
    "    ita=0.002\n",
    "#     x_train,y_train,x_test,y_test,x_valid,y_valid=loadDataSet()\n",
    "    column_num=x_train.shape[1]\n",
    "    train_num=x_train.shape[0]\n",
    "    w=initW(column_num+1)\n",
    "    data_train=normalize(x_train)\n",
    "    for i in range(500):\n",
    "        deltaW=np.zeros(column_num+1)\n",
    "        for t in range(train_num):\n",
    "            row=np.append(1,data_train[t])\n",
    "            y=sigmoid(w.dot(row))\n",
    "            error=y_train.values[t]-y\n",
    "            deltaW+=error*row\n",
    "        w+=ita*deltaW\n",
    "        if test_logisticRegression(w,x_valid,y_valid)>=0.95:\n",
    "            break\n",
    "    return w\n",
    "\n",
    "\n",
    "def logisticRegression_train_and_test(x_train,y_train,x_test,y_test,x_valid,y_valid):\n",
    "    w=logisticRegression(x_train,y_train,x_valid,y_valid)\n",
    "    return test_logisticRegression(w,x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. svm  \n",
    "use module svm in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "\n",
    "def svmPredict(x_train,y_train):\n",
    "#     x_train,y_train,x_test,y_test,x_valid,y_valid=loadDataSet()\n",
    "    clf = svm.SVC(C=1,decision_function_shape='ovo')\n",
    "    clf.fit(x_train,y_train)\n",
    "    return clf\n",
    "\n",
    "\n",
    "def svm_test(clf,x_test,y_test):\n",
    "    return clf.score(x_test,y_test)\n",
    "\n",
    "\n",
    "def svm_train_and_test(x_train,y_train,x_test,y_test,x_valid,y_valid):\n",
    "    clf=svmPredict(x_train,y_train)\n",
    "    return svm_test(clf,x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical Study\n",
    "1. comparison of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def figureTableForAccuracy():\n",
    "    name_list= ['training set','test set']\n",
    "    x_train,y_train,x_test,y_test,x_valid,y_valid=loadDataSet()\n",
    "    bayes=[]\n",
    "    rl=[]\n",
    "    svm=[]\n",
    "\n",
    "    true_mean,true_std,false_mean,false_std,true_data_possibility,false_data_possibility=naive_bayes_predict(x_train,y_train)\n",
    "    bayes.append(test_bayes(true_mean,true_std,false_mean,false_std,true_data_possibility,false_data_possibility,x_train,y_train))\n",
    "    bayes.append(test_bayes(true_mean,true_std,false_mean,false_std,true_data_possibility,false_data_possibility,x_test,y_test))\n",
    "\n",
    "    w=logisticRegression(x_train,y_train,x_valid,y_valid)\n",
    "    rl.append(test_logisticRegression(w,x_train,y_train))\n",
    "    rl.append(test_logisticRegression(w,x_test,y_test))\n",
    "\n",
    "    clf=svmPredict(x_train,y_train)\n",
    "    svm.append(svm_test(clf,x_train,y_train))\n",
    "    svm.append(svm_test(clf,x_test,y_test))\n",
    "\n",
    "    x =list(range(len(bayes))) \n",
    "    total_width, n = 0.6, 3\n",
    "    width = total_width / n\n",
    "\n",
    "    plt.bar(x, bayes, width=width, label='naive-bayes',fc = 'y')  \n",
    "    for i in range(len(x)):  \n",
    "        x[i] = x[i] + width\n",
    "    plt.bar(x, rl, width=width, label='logistic regressio',tick_label = name_list,fc = 'r')\n",
    "    for i in range(len(x)):  \n",
    "        x[i] = x[i] + width\n",
    "    plt.bar(x, svm, width=width, label='svm',fc = 'b')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    table=np.reshape(bayes+rl+svm,)\n",
    "    return pd.DataFrame(runtime,index=['naive-bayes','logistic regression','svm'],columns=['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def displayTime():\n",
    "    x_train,y_train,x_test,y_test,x_valid,y_valid=loadDataSet()\n",
    "    runtime=[]\n",
    "    start =time.time()\n",
    "    bayes_train_and_test(x_train,y_train,x_test,y_test,x_valid,y_valid)\n",
    "    end = time.time()\n",
    "    runtime.append(end-start)\n",
    "\n",
    "    start =time.time()\n",
    "    logisticRegression_train_and_test(x_train,y_train,x_test,y_test,x_valid,y_valid)\n",
    "    end = time.time()\n",
    "    runtime.append(end-start)\n",
    "\n",
    "    start =time.time()\n",
    "    svm_train_and_test(x_train,y_train,x_test,y_test,x_valid,y_valid)\n",
    "    end = time.time()\n",
    "    runtime.append(end-start)\n",
    "\n",
    "    return pd.DataFrame(runtime,index=['naive-bayes','logistic regression','svm'],columns=['time'])\n",
    "\n",
    "\n",
    "test=[1,2,3,4,5,6]\n",
    "result=np.reshape(test,(2,3))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
